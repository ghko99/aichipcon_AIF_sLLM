# backend/chatbot_2/chatbot.py

import numpy as np
from rank_bm25 import BM25Okapi
from konlpy.tag import Okt
import pickle
from FlagEmbedding import BGEM3FlagModel
import tritonclient.grpc.aio as grpcclient
from tqdm.asyncio import tqdm  # ë¹„ë™ê¸° tqdm ì‚¬ìš©
import json
import os
import asyncio
import logging

# ê¸€ë¡œë²Œ ë³€ìˆ˜ ì´ˆê¸°í™”
emb_model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
korean_tokenizer = Okt()
triton_client = None
retriever_dict = {}
logger = logging.getLogger(__name__)
# Retriever Class
class AnnouncementRetriever:
    def __init__(self, elements, emb_model, korean_tokenizer):
        """ RAG ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” """
        self.elements = elements
        self.emb_model = emb_model
        self.korean_tokenizer = korean_tokenizer

        # BM25 ì„¤ì •
        self.chunks_with_title = [e['title'] + '\n' + e['markdown'] for e in self.elements]
        self.tokenized_chunks = [self.korean_tokenizer.morphs(chunk) for chunk in self.chunks_with_title]
        self.bm25 = BM25Okapi(self.tokenized_chunks)

        # ì œëª© ë° ë¬¸ì„œ ì„ë² ë”© ì„¤ì •
        self.title_embeddings = np.array([e['title_emb'] for e in elements], dtype=np.float16)
        self.doc_embeddings = np.array([e['doc_emb'] for e in elements], dtype=np.float16)

        # ê°€ì¤‘ì¹˜ ì„¤ì •
        self.bm25_weights = 0.1
        self.title_similarity_weights = 0.2
        self.doc_similarity_weights = 0.7
        self.lower_bound = 0.5

    def get_bm25_scores(self, query):
        tokenized_query = self.korean_tokenizer.morphs(query)
        bm_scores = self.bm25.get_scores(tokenized_query)
        return bm_scores

    def get_similarity(self, query):
        query_embedding = self.emb_model.encode([query])['dense_vecs']
        title_sim = query_embedding @ self.title_embeddings.T
        doc_sim = query_embedding @ self.doc_embeddings.T
        return (title_sim, doc_sim)

    def min_max_normalize(self, scores):
        if scores.max() == scores.min():
            return np.array([0] * len(scores), dtype=np.float16)
        return (scores - scores.min()) / (scores.max() - scores.min())

    def get_rag_scores(self, query):
        bm25_scores = self.get_bm25_scores(query)
        sims = self.get_similarity(query)
        norm_bm25 = self.min_max_normalize(bm25_scores)
        norm_title = self.min_max_normalize(sims[0][0])
        norm_doc = self.min_max_normalize(sims[1][0])
        rag_scores = self.bm25_weights * norm_bm25 + \
                     self.title_similarity_weights * norm_title + \
                     self.doc_similarity_weights * norm_doc
        return rag_scores

    def get_retriever_results(self, query):
        rag_scores = self.get_rag_scores(query)
        arg_max_idx = rag_scores.argsort()[::-1]
        total_token_len = 0
        idx_with_order = []
        for idx in arg_max_idx:
            if rag_scores[idx] < self.lower_bound:
                break
            token_len = self.elements[idx]['token_len']
            order = self.elements[idx]['id']
            if total_token_len + token_len > 3000:
                break
            idx_with_order.append([order, idx])
            total_token_len += token_len
        idx_with_order.sort()
        cur_title = ''
        total_md_txt = ''
        for idx in idx_with_order:
            title = self.elements[idx[1]]['title']
            md_txt = self.elements[idx[1]]['markdown']
            if title != cur_title:
                total_md_txt = total_md_txt + '\n' + title + '\n'
                cur_title = title
            total_md_txt = total_md_txt + md_txt + '\n'
        return total_md_txt

# ìš”ì†Œ ë¡œë“œ í•¨ìˆ˜ (ë™ì ìœ¼ë¡œ fileidë¥¼ ë°›ì•„ ë¡œë“œ)
def get_elements(fileid):
    base_path = './backend/chatbot/preprocessed_data'
    file_path = os.path.join(base_path, fileid)  # ê²½ë¡œ ìƒì„±
    print(f"Trying to load file: {file_path}")  # ë””ë²„ê¹…ìš© ì¶œë ¥
    with open(file_path, 'rb') as f:
        elements = pickle.load(f)
    return elements

# Prompt ìƒì„± í•¨ìˆ˜
def construct_prompt(question, retrieve_result):
    prompt_template = """
A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
NOTE:
- DO NOT use the Context to generate a response unless the user explicitly refers to it or asks about its content.
- If the user's input is unclear, incomplete, or nonsensical, respond politely by asking for clarification. Example: "ì§ˆë¬¸ì´ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œë²ˆ ì„¤ëª… ë¶€íƒë“œë¦½ë‹ˆë‹¤."
- If the user's input does not relate to the Context, answer only based on general knowledge.
- DO NOT generate answers unrelated to the user's question or intent.
- DO NOT include or create irrelevant content like tables, symbols, or lengthy explanations unless explicitly asked.
- ALWAYS ANSWER IN KOREAN.
Context: {input}
Human: {question}
Assistant:
"""
    return prompt_template.format(question=question, input=retrieve_result)

# Generation ìš”ì²­ ìƒì„± í•¨ìˆ˜
def create_generation_request(prompt, request_id):
    input_tensor = grpcclient.InferInput("text_input", [1], "BYTES")
    prompt_data = np.array([prompt.encode("utf-8")])
    input_tensor.set_data_from_numpy(prompt_data)

    stream_setting = grpcclient.InferInput("stream", [1], "BOOL")
    stream_setting.set_data_from_numpy(np.array([True]))

    # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì¶”ê°€
    sampling_parameters = {
        "max_tokens": 256,
        "temperature": 0.1,
        "top_p": 0.95,
        "repetition_penalty": 1.2,
    }
    sampling_parameters_input = grpcclient.InferInput("sampling_parameters", [1], "BYTES")
    sampling_parameters_data = np.array([json.dumps(sampling_parameters).encode("utf-8")], dtype=object)
    sampling_parameters_input.set_data_from_numpy(sampling_parameters_data)

    inputs = [input_tensor, stream_setting, sampling_parameters_input]

    output = grpcclient.InferRequestedOutput("text_output")
    outputs = [output]

    return {
        "model_name": "vllm_model",
        "inputs": inputs,
        "outputs": outputs,
        "request_id": request_id
    }

# ë¹„ë™ê¸° ì´ˆê¸°í™” í•¨ìˆ˜
async def async_init():
    global triton_client, retriever_dict

    url = "localhost:8001"  # Triton inference server URL
    print("Initializing Triton client...")
    triton_client = grpcclient.InferenceServerClient(url=url, verbose=False)
    await triton_client.is_server_ready()

    # ìŠ¤í¬ë¦½íŠ¸ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
    base_path = os.path.join(os.path.dirname(__file__), '..', 'preprocessed_data')
    base_path = os.path.abspath(base_path)
    print(f"Loading preprocessed data from: {base_path}")  # ë””ë²„ê¹…ìš© ì¶œë ¥

    # ëª¨ë“  retriever load (ë¹„ë™ê¸°ì ìœ¼ë¡œ íŒŒì¼ ë¡œë“œ)
    try:
        files = os.listdir(base_path)
    except FileNotFoundError:
        print(f"Directory not found: {base_path}")
        raise

    retrievers = []
    for file in tqdm(files):
        # íŒŒì¼ ë¡œë“œëŠ” ë™ê¸°ì ìœ¼ë¡œ ìˆ˜í–‰ (pickleì€ ë™ê¸°ì )
        elements = await asyncio.to_thread(get_elements, os.path.join(base_path, file))
        retriever = AnnouncementRetriever(elements=elements, emb_model=emb_model, korean_tokenizer=korean_tokenizer)
        retrievers.append((retriever, file))

    # file idì™€ retriever ê°ì²´ë“¤ mapping
    retriever_dict = {retrieve[1]: retrieve[0] for retrieve in retrievers}
    print("Initialization complete.")
    print("fycj")

# retrieve_and_generate í•¨ìˆ˜# retrieve_and_generate í•¨ìˆ˜
async def retrieve_and_generate(fileid, user_input):
    # Step 1: ë™ì ìœ¼ë¡œ ìš”ì†Œ ë¡œë“œ ë° retriever ìƒì„±
    retriever = retriever_dict[fileid]

    # Step 2: Retrieve context using RAG retriever
    retrieve_result = retriever.get_retriever_results(user_input)
    logger.info(f"Retrieve result: {retrieve_result}")  # print ëŒ€ì‹  logger ì‚¬ìš©    
    # Step 3: Construct prompt for generation
    prompt = construct_prompt(user_input, retrieve_result)

    async def requests_gen():
        yield create_generation_request(prompt, "req-0")

    # 'await' ì œê±°
    response_stream = triton_client.stream_infer(requests_gen())
    previous_output = ""
    full_output = ""
    # Step 4: Stream the response from Triton server
    async for response in response_stream:
        result, error = response
        if error:
            yield f"Error occurred! Details: {str(error)}"
        else:
            output = result.as_numpy("text_output")
            for i in output:
                decoded = i.decode("utf-8")

                # Remove prompt part if present
                if decoded.startswith(prompt):
                    decoded = decoded[len(prompt):]
                # Remove duplicate part from new output
                new_output = decoded[len(previous_output):]
                logger.debug(f"New output before spacing: '{new_output}'")

                # Update the previous output for tracking
                previous_output += new_output

                # ë§ˆí¬ë‹¤ìš´ ì¤„ë°”ê¿ˆ ë° ë„ì–´ì“°ê¸° ì²˜ë¦¬
                # ìƒˆë¡œìš´ ì¶œë ¥ì´ íŠ¹ì • ë¬¸ì¥ìœ¼ë¡œ ì‹œì‘í•  ë•Œ ì¤„ë°”ê¿ˆì„ ì¶”ê°€
                if full_output and not full_output.endswith(('\n', '<br>')):
                    # ì˜ˆ: ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë“±ìœ¼ë¡œ ëë‚˜ì§€ ì•Šìœ¼ë©´ ì¤„ë°”ê¿ˆ ì¶”ê°€
                    if full_output[-1] in ('.', '!', '?'):
                        new_output = '\n\n' + new_output
                    else:
                        new_output = ' ' + new_output
                else:
                    new_output = new_output

                # ê°•ì œ ì¤„ë°”ê¿ˆì„ ìœ„í•´ <br> íƒœê·¸ ì¶”ê°€
                # í•„ìš”ì— ë”°ë¼ íŠ¹ì • ìœ„ì¹˜ì— <br> íƒœê·¸ë¥¼ ì‚½ì…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                # ì˜ˆì‹œ: ë¬¸ì¥ ëì— <br> ì¶”ê°€
                if new_output.endswith(('.', '!', '?')):
                    new_output += '  \n'  # ë§ˆí¬ë‹¤ìš´ì—ì„œ ë‘ ê°œì˜ ê³µë°±ê³¼ ì—”í„°ëŠ” ì¤„ë°”ê¿ˆì„ ì˜ë¯¸

                # full_outputì— new_output ì¶”ê°€
                full_output += new_output
                logger.debug(f"New output after spacing: '{new_output}'")

                # ì§ì ‘ ì²­í¬ë¥¼ ë°˜í™˜
                yield new_output
    logger.debug(f"Final output with markdown: '{full_output}'")

    # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œë¥¼ ì•Œë¦¬ëŠ” ë©”ì‹œì§€
    yield "ğŸ˜ƒ ì´ìƒ AIF LAB ì´ì—ˆìŠµë‹ˆë‹¤. \n ë” ê¶ê¸ˆí•˜ì‹ ì  ìˆë‚˜ìš”?"